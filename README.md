# Subject-driven generating using Dreambooth

## Subject-driven generation
 
Диффузионные модели достигли state-of-the-art результатов в области генерации изображений. Однако обычные диффузионные модели практически не могут сгенерировать один и тот же объект дважды. 
Это сильно ограничивает диапозон их применения. Так, например, умей модель запоминать по некоторым референсным изображениям персонажа, она могла бы сильно сократить работу художникам и ускорить производство контента (например, рисовать спрайты в видеоиграх). Также модель можно было бы всё с теми же целями внедрить в графический редактор.
Чтобы решить эту проблему исслудуюься методы для subject-driven generation объектов. Вкратце проблематику данной задачи можно сформулировать следующим образом: генерация объекта в новых условиях с помощью дообученной на небольшом датасете, сохраняя при этом харакетные черты объекта


## Dreambooth

DreamBooth - метод, разработанный гуглом для решения поставленной выше задачи.
Основаная идея заключается в том, чтобы обучить text-to-image модели уникальный токен объекта с его визуальным представлением.
В работе можно выделить следующие особенности:

<ol>
<li> В качестве уникального идентификатора используя токены, которые были наиболее редкими на этапе обучения, для того чтобы исключить корреляцию идентефекатора с какими либо другими объектами или явлениями. </li>
<li> Для того, чтобы получить некоторые априорные знания об таргетном объекте, авторы статьи вместе с уникальным токеном в промте подают и метку класса, полученную после классификации объекта</li>
<li> Чтобы исключить language-drift, авторы используют также Prior Preservation Loss, который позволяет сохранить начальное распределение. Работает этот лосс следующим образом: мы мерием расстояние между картинками, сгенерированными с помощью промпта без токена на дообучаемой модели и изначальной модели  </li>
</ol>   

В результате своих экспериментов, авторы получили хорошие результаты: в большинстве примеров сохранялись признаки объекта с референсных изображений (например, у собаки ниже сохранилась асимметричная линия). Единственное, с чем не справлялась модель - это с генерацией объекта в неестественной для него среде, но это скорее пробелма не метода, а данных, на которых обучалась изначальная модель

## Эксперимент

[Ссылка на Colab](https://colab.research.google.com/drive/1t-YQ41rhpFrnrdJB2vg3aOPly5RRriJV?usp=sharing):


В ходе эксперимента была дообучена модель Stable Diffusion 1.5, на небольшом датасете, состоящем из 6 элементов. Я опробовал генерацию на двух предметах: первый - это моя кошка, второй - мои очки.

Модель обучалась в течении 4500 шагов с шагом 0.003, и с фиксированным сидом.

Для каждого объекта я сначала варьировал скейл у Текст-энкодера и скейл у Юнета. В качестве guidance был выставлено значение 5, поскольу я немного экспериментировал на мой субъектвный взгляд такое значение параметра оказалось самым подходящим. Количество шагов на инференсе было взято равное 200. Таким образом, варьируя параметры я смотрел, как ведёт себя модель с базовым промтом "ktn cat", где 'ktn' - уникальный идентифиактор для объекта. К сожалению мне не удалось воспроизвести результаты, полученные авторами. Возможно, это может быть свзяано с различными гиперпараметрами, используемыми на этапе обучения и инференса. Для датасета с очками ситуация аналогичная.
